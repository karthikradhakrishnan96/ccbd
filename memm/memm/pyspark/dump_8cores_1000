Loading input
Done
Preprocessing for gradient
Done
before training:  2015-04-24 00:28:28.196922
[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] cost params
--------------------------------------------------
95301.0317406 95301.0317406 0.0
--------------------------------------------------
2597.0 165.136363636
1054.0 1054.0
1182.0 151.272727273
175.0 82.7272727273
2151.0 62.8863636364
165.0 13.6363636364
533.0 18.2727272727
421.0 16.2045454545
1944.0 53.5681818182
12.0 0.272727272727
392.0 8.95454545455
517.0 11.7954545455
0.0 0.0
183.0 4.25
--------------------------------------------------
[  6.11712968e-01   5.71937603e-17   2.59269981e-01   2.32103573e-02
   5.25245812e-01   3.80741329e-02   1.29474919e-01   1.01822580e-01
   4.75520766e-01   2.94988777e-03   9.63515670e-02   1.27079565e-01
   0.00000000e+00   4.49629212e-02] cost params
--------------------------------------------------
91384.1581319 95567.4049845 4183.24685264
--------------------------------------------------
2597.0 294.56598956
1054.0 1054.0
1182.0 194.432118735
175.0 84.2177782276
2151.0 104.344355088
165.0 14.033612487
533.0 20.5444939201
421.0 17.8816367523
1944.0 84.6458487407
Traceback (most recent call last):
  File "/home/pratheek/work/memm/pyspark/memm_pyspark.py", line 411, in <module>
    params = mymin(cost1, param, method = 'L-BFGS-B', jac = gradient1_new, options = {'maxiter':100}) #, jac = self.gradient) # , options = {'maxiter':100}
  File "/usr/lib/python2.7/site-packages/scipy/optimize/_minimize.py", line 427, in minimize
    callback=callback, **options)
  File "/usr/lib/python2.7/site-packages/scipy/optimize/lbfgsb.py", line 315, in _minimize_lbfgsb
    f, g = func_and_grad(x)
  File "/usr/lib/python2.7/site-packages/scipy/optimize/lbfgsb.py", line 267, in func_and_grad
    g = jac(x, *args)
  File "/home/pratheek/work/memm/pyspark/memm_pyspark.py", line 366, in gradient1_new
    expected = distributed_input_data.map(helper).reduce(lambda x,y:x+y)
  File "/home/pratheek/work/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py", line 740, in reduce
    vals = self.mapPartitions(func).collect()
  File "/home/pratheek/work/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py", line 701, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/home/pratheek/work/spark-1.3.0-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 536, in __call__
  File "/home/pratheek/work/spark-1.3.0-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 364, in send_command
  File "/home/pratheek/work/spark-1.3.0-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py", line 473, in send_command
  File "/usr/lib/python2.7/socket.py", line 430, in readline
    data = recv(1)
KeyboardInterrupt
